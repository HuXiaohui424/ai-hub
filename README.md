# AI Hub

## 提示词工程

不同的提示词能够让大模型给出差异巨大的答案。
不断雕琢提示词，使大模型能给出最理想的答案，这个过程就叫做**提示词工程**（Prompt Engineering）。

很多简单的AI应用，仅仅靠一段足够好的提示词就能实现了，这就是纯Prompt模式。

> 案例：哄哄模拟器基于纯Prompt实现



## FunctionCalling

大模型虽然可以理解自然语言，更清晰弄懂用户意图，但是却无法直接操作数据库、执行严格的业务规则。这个时候我们就可以整合传统应用于大模型的能力了。

简单来说，可以分为以下步骤：

1. 我们可以把传统应用中的部分功能封装成一个个函数（Function）。
2. 然后在提示词中描述用户的需求，并且描述清楚每个函数的作用，要求AI理解用户意图，判断什么时候需要调用哪个函数，并且将任务拆解为多个步骤（Agent）。
3. 当AI执行到某一步，需要调用某个函数时，会返回要调用的函数名称、函数需要的参数信息。
4. 传统应用接收到这些数据以后，就可以调用本地函数。再把函数执行结果封装为提示词，再次发送给AI。
5. 以此类推，逐步执行，直到达成最终结果。

>案例：智能客服基于FunctionCalling实现



## RAG

RAG（**R**etrieval**-A**ugmented **G**eneration）叫做检索增强生成。简单来说就是把**信息检索技术**和**大模型**结合的方案。

大模型从知识角度存在很多限制：

- **时效性差**：大模型训练比较耗时，其训练数据都是旧数据，无法实时更新
- **缺少专业领域知识**：大模型训练数据都是采集的通用数据，缺少专业数据
- **上下文大小限制：**大模型所支持的上下文内容（token）是有限的

**解决方案：**

RAG就是利用信息检索技术来拓展大模型的知识库，解决大模型的知识限制。整体来说RAG分为两个模块：

- **检索模块（Retrieval）**：负责存储和检索拓展的知识库
  - 文本拆分：将文本按照某种规则拆分为很多片段
  - 文本嵌入（Embedding)：根据文本片段内容，将文本片段归类存储
  - 文本检索：根据用户提问的问题，找出最相关的文本片段
- **生成模块（Generation）**：
  - 组合提示词：将检索到的片段与用户提问组织成提示词，形成更丰富的上下文信息
  - 生成结果：调用生成式模型（例如DeepSeek）根据提示词，生成更准确的回答

由于每次都是从向量库中找出与用户问题相关的数据，而不是整个知识库，所以上下文就不会超过大模型的限制，同时又保证了大模型回答问题是基于知识库中的内容，完美！

>案例：ChatPDF就是基于RAG实现的



## 其他

1. AI聊天案例是直接调用大模型所暴露的API接口，当前项目所调用的大模型主要是阿里云百炼。
2. 项目中Ollama平台没有使用到，因为deepseek-r1模型不支持多模态，AI聊天案例是支持多模态的。
3. 项目中使用的向量数据库（simpleVectorStore)是基于内存存储的，不适用于实际的生产项目。
4. 配置文件中的openai下的api-key需要替换为自己阿里百炼平台的api-key，**否则项目无法启动**。
……

